
### STREAMING EXPERIMENTS

To run the experiments, execute the bash scripts in the following order:

cd /srv/intel_streamingbenchmark

# Create the 'tweets' and 'AB' topics in Kafka
./bin/create-kafka-spark.sh

# Generate the input data for the streaming experiments
./bin/streaming-generate.sh

# In the backgroud, start kafka clients that read the kafka latencies from the output Spark and Flink topics and write the latencies to an output file for each of Spark and Flink
./bin/consume-kafka-flink.sh
./bin/consume-kafka-spark.sh


# Flink streaming experiments
./bin/streaming-tweets.sh
./bin/streaming-ab.sh


# Spark streaming experiments
./bin/spark-streaming-tweets.sh
./bin/spark-streaming-ab.sh


# Process the output files name *-flink.out and *-spark.out


### BATCH EXPERIMENTS

# You will need to stop flink jobmanager that is running in streaming mode by default. Then restart it in batch mode.

cd /srv/flink
./bin/stop-cluster.sh
./bin/start-cluster.sh

# To later start the flink cluster in 'streaming' mode, run ./bin/start-cluster-streaming.sh

# Generate the input data for the batch experiments
cd /srv/intel_streamingbenchmark
./bin/batch-generate.sh

# Run the flink the batch experiments
./bin/batch-als.sh
./bin/batch-pearson.sh


# Run the flink the spark experiments
./bin/spark-batch-als.sh
./bin/spark-batch-pearson.sh
